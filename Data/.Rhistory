slice_max(order_by = n, n = 5) %>%
ggplot() +
geom_col(aes(x = reorder(lemma, n), y = n, fill = lemma)) +
labs(x = "Lemma", y = "Frequency", title = "") +
coord_flip() +
theme(legend.position = "none", text = element_text(size = 18))
dev.off()
# Analisis de sentimiento
# Importacion diccionario AFINN
lexico_afinn <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/lexico_afinn.csv",
col_types = cols(word = col_skip()))
# como es una traduccion hay palabras repetidas por lo tanto se hace un promedio de los puntajes
lexico_afinn<-lexico_afinn %>%group_by(palabra)%>%summarise(puntuacion=mean(puntuacion))
positive_words <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/positive_words_es.txt", col_names = "word", show_col_types = FALSE) %>%
mutate(sentiment = "Positivo")
negative_words <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/negative_words_es.txt", col_names = "word", show_col_types = FALSE) %>%
mutate(sentiment = "Negativo")
sentiment_words <- bind_rows(positive_words, negative_words)
tidy_subtitles_annotated2<-tidy_subtitles_annotated%>%left_join(lexico_afinn,by=c('Token'='palabra'))
str(tidy_subtitles_annotated2)
AFIN<-tidy_subtitles_annotated2%>%select(doc_id,Token,puntuacion)
# hacer el join con el df de palabras negativas y positivas
AFIN<-AFIN%>%left_join(sentiment_words, by=c('Token'='word'))
AFIN<-AFIN %>%
filter(!is.na(puntuacion) | !is.na(sentiment)) %>%
mutate(sentiment=case_when(puntuacion > 0 & is.na(sentiment) ~ "Positivo",
puntuacion < 0 & is.na(sentiment) ~ "Negativo",
puntuacion == 0 & is.na(sentiment) ~ "Neutro",
TRUE ~ sentiment))
AFIN2<-AFIN%>%
group_by(doc_id, sentiment) %>%
summarise(media = mean(puntuacion,na.rm = T), frecuencia=n())
AFIN2%>%filter(sentiment=='Neutro')
AFIN_negativo<-AFIN2%>%filter(sentiment=='Negativo' & !is.na(media))
AFIN_positivo <- AFIN2 %>%
filter(!is.na(media)) %>%
filter(sentiment == 'Positivo' & doc_id!='#jep #caso03 #falsospositivosantioquia') %>%
as.data.frame()
#jep #caso03 #falsospositivosantioquia este video se analiso en el caso de antioquia
png("C:/Users/Pc/Desktop/-Proyecto-JEP-/Outputs/Analisis-Sentimientos/General/hist_score.png",
width = 12, height = 8, units = "cm", res = 300, pointsize = 8, bg = "white")
# Crear un histograma de sentimientos positivos
hist(AFIN_positivo$media, main = "",
xlab = "Mean Sentiment Score", ylab = "Frequency",
prob = TRUE, col = rgb(0.2, 0.6, 0.8, 0.5), ylim = c(0, 3.5), xlim = c(0.7, 2.4 + 0.5),
border = "transparent")
# Añadir la densidad de los sentimientos positivos
lines(density(AFIN_positivo$media), col = "blue", lwd = 2)
# Añadir el histograma de sentimientos negativos sobre el mismo gráfico
hist((-1) * AFIN_negativo$media, prob = TRUE, col = rgb(1, 0.2, 0.2, 0.5),
border = "transparent", add = TRUE)
# Añadir la densidad de los sentimientos negativos
lines(density((-1) * AFIN_negativo$media), col = "red", lwd = 2)
# Añadir leyenda para identificar los sentimientos positivos y negativos
legend("topright", legend = c("Positive", "Negative"),
col = c("blue", "red"), lwd = 2, lty = 1, bty = "n", cex = 1)  # Ajusta y.intersp para reducir espacio vertical
dev.off()
#Tomar el negativo de MN
MN_negativo <- (-1)*AFIN_negativo$media
MP <- AFIN_positivo$media
names(MN_negativo) <- NULL
# Crear un data frame para ggplot
data <- data.frame(x = 1:length(MP), MP = MP, MN = MN_negativo)
png("C:/Users/Pc/Desktop/-Proyecto-JEP-/Outputs/Analisis-Sentimientos/General/serie_score.png",
width = 12, height = 8, units = "cm", res = 300, pointsize = 8, bg = "white")
ggplot(data, aes(x = x)) +
geom_line(aes(y = MN, color = "Negativo"), lwd = 0.5) +
geom_line(aes(y = MP, color = "Positivo"), lwd = 0.5) +
labs(x = "Index", y = "Mean Sentiment Score", title = "") +
scale_color_manual(name = "Sentiment", labels = c("Negative", "Positive"),
values = c("Negativo" = "2", "Positivo" = "4")) +
theme_minimal() +
theme(legend.position = "right",
plot.title = element_text(hjust = 0.5))
dev.off()
#### PRUEBAS DE BONDA DE AJUSTE PARA LA MEDIA
# PRUEBA DE NORMALIDAD PARA MP
shapiro_test_MP <- shapiro.test(MP)
# PRUEBA DE NORMALIDAD PARA MN
shapiro_test_MN <- shapiro.test(MN_negativo)
# Prueba de Mann-Whitney-Wilcoxon
wilcoxon_test <- wilcox.test(MN_negativo, MP, alternative = "greater",paired = T)
# Cargar la librería car para la prueba de Levene
# install.packages("car") # Si no tienes la librería
library(car)
# Prueba de Levene para igualdad de varianzas
levene_test <- leveneTest(c(MP, MN_negativo) ~ factor(c(rep(1, length(MP)), rep(2, length(MN_negativo)))))
# Prueba t de Student (para igualdad de medias)
t_test <- t.test(MN_negativo, MP, alternative = "greater", var.equal = TRUE,paired = T)
t_test2 <- t.test(MN_negativo, MP, alternative = "greater", var.equal = TRUE,paired = F)
# Calcular medias y medianas
media_MP <- mean(MP)
media_MN <- mean(MN_negativo)
mediana_MP <- median(MP)
mediana_MN <- median(MN_negativo)
t_test2 <- t.test(MN_negativo, MP, alternative = "greater", var.equal = F,paired = T)
# Calcular medias y medianas
media_MP <- mean(MP)
media_MN <- mean(MN_negativo)
mediana_MP <- median(MP)
mediana_MN <- median(MN_negativo)
# Crear un data frame para almacenar todos los resultados
resultados_totales <- data.frame(
Estadística = c("Media MP", "Media MN",
"Mediana MP", "Mediana MN",
"Shapiro-Wilk MP P-valor", "Shapiro-Wilk MN P-valor",
"Mann-Whitney W", "Mann-Whitney P-valor",
"Prueba Levene F", "Prueba Levene P-valor",
"Prueba t T var.equa", "Prueba t P-valor var.equa",
"Prueba t T", "Prueba t P-valor"),
Valor = c(media_MP, media_MN,
mediana_MP, mediana_MN,
shapiro_test_MP$p.value, shapiro_test_MN$p.value,
wilcoxon_test$statistic, wilcoxon_test$p.value,
levene_test$"F value"[1], levene_test$"Pr(>F)"[1],
t_test$statistic, t_test$p.value,
t_test2$statistic, t_test2$p.value)
)
# Redondear los valores en el data frame
resultados_totales$Valor <- round(resultados_totales$Valor, 3)
resultados_totales$Valor
# Imprimir el resultado en una tabla con encabezado
cat("Resumen de pruebas estadísticas (MP vs MN_negativo):\n")
print(resultados_totales, row.names = FALSE)
## PROPORCION GRAFICA 1
PP <- AFIN_positivo$frecuencia/(AFIN_positivo$frecuencia+AFIN_negativo$frecuencia)
PN <- AFIN_negativo$frecuencia/(AFIN_positivo$frecuencia+AFIN_negativo$frecuencia)
# Crear densidades kernel para PP y PN
densidad_PP <- density(PP)
densidad_PN <- density(PN)
# Graficar la densidad de Kernel de PP con color azul
png("C:/Users/Pc/Desktop/-Proyecto-JEP-/Outputs/Analisis-Sentimientos/General/dens_proportion.png",
width = 12, height = 8, units = "cm", res = 300, pointsize = 8, bg = "white")
plot(densidad_PN, main = "",
xlab = "Proportion", ylab = "Density",
col = rgb(1, 0.2, 0.2, 0.5), lwd = 2)
# Add vertical lines at the mean of PP and PN
abline(v = mean(PP), col = "blue", lwd = 2, lty = 2)
abline(v = mean(PN), col = "red", lwd = 2, lty = 2)
# Add legend
legend("topright", legend = c("Positive", "Negative"),
col = c("blue", "red"), lwd = 2, lty = 2,
bty = "n", cex = 1)
dev.off()
# Realizar la prueba de Mann-Whitney-Wilcoxon para comparar las proporciones de PP y PN
mann_whitney_test_PP_PN <- wilcox.test(PN, PP, alternative = "greater")
# Realizar la prueba t de Student para comparar las medias de PP y PN
t_test_PP_PN <- t.test(PN, PP, alternative = "greater", var.equal = FALSE)
#### PRUEBAS DE BONDA DE AJUSTE PARA LA MEDIA
# PRUEBA DE NORMALIDAD PARA PP
shapiro_test_PP <- shapiro.test(PP)
# PRUEBA DE NORMALIDAD PARA PN
shapiro_test_PN <- shapiro.test(PN)
# Prueba de Mann-Whitney-Wilcoxon
wilcoxon_test <- wilcox.test(PN, PP, alternative = "greater",paired = T)
# Cargar la librería car para la prueba de Levene
# install.packages("car") # Si no tienes la librería
library(car)
# Prueba de Levene para igualdad de varianzas
levene_test <- leveneTest(c(PP, PN) ~ factor(c(rep(1, length(PP)), rep(2, length(PN)))))
# Prueba t de Student (para igualdad de medias)
t_test <- t.test(PN, PP, alternative = "greater", var.equal = TRUE,paired = T)
t_test2 <- t.test(PN, PP, alternative = "greater", var.equal = F,paired = T)
# Calcular medias y medianas
media_PP <- mean(PP)
media_PN <- mean(PN)
mediana_PP <- median(PP)
mediana_PN <- median(PN)
# Crear un data frame para almacenar todos los resultados
resultados_totales <- data.frame(
Estadística = c("Media PP", "Media PN",
"Mediana PP", "Mediana PN",
"Shapiro-Wilk PP P-valor", "Shapiro-Wilk PN P-valor",
"Mann-Whitney W", "Mann-Whitney P-valor",
"Prueba Levene F", "Prueba Levene P-valor",
"Prueba t T var.equa", "Prueba t P-valor var.equa",
"Prueba t T", "Prueba t P-valor"),
Valor = c(media_PP, media_PN,
mediana_PP, mediana_PN,
shapiro_test_PP$p.value, shapiro_test_PN$p.value,
wilcoxon_test$statistic, wilcoxon_test$p.value,
levene_test$"F value"[1], levene_test$"Pr(>F)"[1],
t_test$statistic, t_test$p.value,
t_test2$statistic, t_test2$p.value)
)
# Redondear los valores en el data frame
resultados_totales$Valor <- round(resultados_totales$Valor, 3)
# Imprimir el resultado en una tabla con encabezado
cat("Resultados de la comparación de proporciones (PN vs PN):\n")
print(resultados_totales, row.names = FALSE)
save.image("C:/Users/Pc/Desktop/-Proyecto-JEP-/Scripts/GENERAL/rgeneral.RData")
#### PRUEBAS DE BONDA DE AJUSTE PARA LA MEDIA
# PRUEBA DE NORMALIDAD PARA MP
shapiro_test_MP <- shapiro.test(MP)
# PRUEBA DE NORMALIDAD PARA MN
shapiro_test_MN <- shapiro.test(MN_negativo)
# Pruebas de Diferencia de Medias y Medianas
# Prueba de Mann-Whitney-Wilcoxon
wilcoxon_test <- wilcox.test(MN_negativo, MP, alternative = "greater",paired = T)
# Cargar la librería car para la prueba de Levene
# install.packages("car") # Si no tienes la librería
library(car)
# Prueba de Levene para igualdad de varianzas
levene_test <- leveneTest(c(MP, MN_negativo) ~ factor(c(rep(1, length(MP)), rep(2, length(MN_negativo)))))
# Prueba t de Student (para igualdad de medias)
t_test <- t.test(MN_negativo, MP, alternative = "greater", var.equal = TRUE,paired = T)
t_test2 <- t.test(MN_negativo, MP, alternative = "greater", var.equal = F,paired = T)
# Calcular medias y medianas
media_MP <- mean(MP)
media_MN <- mean(MN_negativo)
mediana_MP <- median(MP)
mediana_MN <- median(MN_negativo)
# Crear un data frame para almacenar todos los resultados
resultados_totales <- data.frame(
Estadística = c("Media MP", "Media MN",
"Mediana MP", "Mediana MN",
"Shapiro-Wilk MP P-valor", "Shapiro-Wilk MN P-valor",
"Mann-Whitney W", "Mann-Whitney P-valor",
"Prueba Levene F", "Prueba Levene P-valor",
"Prueba t T var.equa", "Prueba t P-valor var.equa",
"Prueba t T", "Prueba t P-valor"),
Valor = c(media_MP, media_MN,
mediana_MP, mediana_MN,
shapiro_test_MP$p.value, shapiro_test_MN$p.value,
wilcoxon_test$statistic, wilcoxon_test$p.value,
levene_test$"F value"[1], levene_test$"Pr(>F)"[1],
t_test$statistic, t_test$p.value,
t_test2$statistic, t_test2$p.value)
)
# Redondear los valores en el data frame
resultados_totales$Valor <- round(resultados_totales$Valor, 3)
# Imprimir el resultado en una tabla con encabezado
cat("Resumen de pruebas estadísticas (MP vs MN_negativo):\n")
print(resultados_totales, row.names = FALSE)
#### PRUEBAS DE BONDA DE AJUSTE PARA LA MEDIA
# PRUEBA DE NORMALIDAD PARA PP
shapiro_test_PP <- shapiro.test(PP)
# PRUEBA DE NORMALIDAD PARA PN
shapiro_test_PN <- shapiro.test(PN)
# Pruebas de Diferencia de Medias y Medianas
# Prueba de Mann-Whitney-Wilcoxon
wilcoxon_test <- wilcox.test(PN, PP, alternative = "greater",paired = T)
# Cargar la librería car para la prueba de Levene
# install.packages("car") # Si no tienes la librería
library(car)
# Prueba de Levene para igualdad de varianzas
levene_test <- leveneTest(c(PP, PN) ~ factor(c(rep(1, length(PP)), rep(2, length(PN)))))
# Prueba t de Student (para igualdad de medias)
t_test <- t.test(PN, PP, alternative = "greater", var.equal = TRUE,paired = T)
t_test2 <- t.test(PN, PP, alternative = "greater", var.equal = F,paired = T)
# Calcular medias y medianas
media_PP <- mean(PP)
media_PN <- mean(PN)
mediana_PP <- median(PP)
mediana_PN <- median(PN)
# Crear un data frame para almacenar todos los resultados
resultados_totales <- data.frame(
Estadística = c("Media PP", "Media PN",
"Mediana PP", "Mediana PN",
"Shapiro-Wilk PP P-valor", "Shapiro-Wilk PN P-valor",
"Mann-Whitney W", "Mann-Whitney P-valor",
"Prueba Levene F", "Prueba Levene P-valor",
"Prueba t T var.equa", "Prueba t P-valor var.equa",
"Prueba t T", "Prueba t P-valor"),
Valor = c(media_PP, media_PN,
mediana_PP, mediana_PN,
shapiro_test_PP$p.value, shapiro_test_PN$p.value,
wilcoxon_test$statistic, wilcoxon_test$p.value,
levene_test$"F value"[1], levene_test$"Pr(>F)"[1],
t_test$statistic, t_test$p.value,
t_test2$statistic, t_test2$p.value)
)
# Redondear los valores en el data frame
resultados_totales$Valor <- round(resultados_totales$Valor, 3)
# Imprimir el resultado en una tabla con encabezado
cat("Resultados de la comparación de proporciones (PN vs PN):\n")
print(resultados_totales, row.names = FALSE)
# Librerias
# lematizador udpipe
library(udpipe)
library(stringr)
library(tibble)
library(readr)
library(ggplot2)
library(wordcloud)
library(dplyr)
library(tidyr)
library(tidyverse)
library(tidytext)
library(magrittr)
library(igraph)
library(ngram)
library(xtable)
stop_words_es<-read.table("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/stop_words_spanish.txt", quote="\"", comment.char="")
FilPal <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/FilPal.txt", col_names = FALSE)
stop_words_es<-c(stop_words_es$V1,FilPal$X1)
stop_words_es <- tibble(word = unlist(stop_words_es), lexicon = "custom")
# Leer el archivo línea por línea
correcciones <- readLines("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/Grupos de Palabra/palabras_correcciones.txt")
# Leer el archivo línea por línea
correcciones <- readLines("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/Grupos de Palabras/palabras_correcciones.txt")
# Separar las palabras originales de sus correcciones
correcciones_df <- as.data.frame(do.call(rbind, strsplit(correcciones, " - ")), stringsAsFactors = FALSE)
names(correcciones_df) <- c("Palabra_Original", "Correccion")
correcciones_df <-correcciones_df%>%
mutate(Correccion = str_replace(Correccion, "^NA$", ""))
# lematizador udpipe
library(udpipe)
#
library(stringr)
library(tibble)
library(readr)
library(ggplot2)
library(wordcloud)
library(dplyr)
library(tidyr)
library(tidyverse)
library(tidytext)
library(magrittr)
library(igraph)
library(ngram)
library(xtable)
#######################################################################################
setwd("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data")
subtitulos_JEP_03 <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Scraping/subtitulos_JEP_03.csv")
stop_words_es<-read.table("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/stop_words_spanish.txt", quote="\"",
comment.char="")
FilPal <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/FilPal.txt", col_names = FALSE)
# Leer el archivo línea por línea
correcciones <- readLines("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/Grupos de Palabras/palabras_correcciones.txt")
# Separar las palabras originales de sus correcciones
correcciones_df <- as.data.frame(do.call(rbind, strsplit(correcciones, " - ")), stringsAsFactors = FALSE)
names(correcciones_df) <- c("Palabra_Original", "Correccion")
correcciones_df <-correcciones_df%>%
mutate(Correccion = str_replace(Correccion, "^NA$", ""))%>%
filter(Correccion=="")
stop_words_es<-c(stop_words_es$V1,FilPal$X1,correcciones_df$Palabra_Original)
stop_words_es <- tibble(word = unlist(stop_words_es), lexicon = "custom")
subtitulos_JEP_03<-subtitulos_JEP_03%>%
mutate(Subtitulos= str_replace_all(Subtitulos, "\\[Música\\]", ""))  %>%
filter(!is.na(Subtitulos) ) %>%
as.data.frame()
# Preprocesar los subtítulos
subtitulos_JEP_03 <- within(subtitulos_JEP_03, {
Subtitulos <- str_to_lower(Subtitulos)  # Convertir a minúsculas
Subtitulos <- str_replace_all(Subtitulos, "[^a-záéíóúüñ ]", "")  # Eliminar caracteres no alfabéticos
Subtitulos <- str_squish(Subtitulos)  # Eliminar espacios en blanco adicionales
})
#correr esta linea para hacer subcasos
subtitulos= paste(subtitulos_JEP_03$Subtitulos, collapse = ' ')
subtitulos <- tibble(text = subtitulos)
subtitulos <- subtitulos %>%
unnest_tokens(input = text, output = word, token = "regex", pattern = "\\s+")%>%
anti_join(x = ., y = stop_words)
# Extract tokens
tokens <- subtitulos$word
# Create bigrams from tokens and avoid multiple edges
bigrams <- sapply(1:(length(tokens) - 1), function(i) {
words <- sort(c(tokens[i], tokens[i + 1]))
paste(words, collapse = " ")
})
stop_words_es
# lematizador udpipe
library(udpipe)
#
library(stringr)
library(tibble)
library(readr)
library(ggplot2)
library(wordcloud)
library(dplyr)
library(tidyr)
library(tidyverse)
library(tidytext)
library(magrittr)
library(igraph)
library(ngram)
library(xtable)
#######################################################################################
setwd("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data")
subtitulos_JEP_03 <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Scraping/subtitulos_JEP_03.csv")
stop_words_es<-read.table("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/stop_words_spanish.txt", quote="\"",
comment.char="")
FilPal <- read_csv("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/FilPal.txt", col_names = FALSE)
# Leer el archivo línea por línea
correcciones <- readLines("C:/Users/Pc/Desktop/-Proyecto-JEP-/Data/Grupos de Palabras/palabras_correcciones.txt")
# Separar las palabras originales de sus correcciones
correcciones_df <- as.data.frame(do.call(rbind, strsplit(correcciones, " - ")), stringsAsFactors = FALSE)
names(correcciones_df) <- c("Palabra_Original", "Correccion")
correcciones_df <-correcciones_df%>%
mutate(Correccion = str_replace(Correccion, "^NA$", ""))%>%
filter(Correccion=="")
stop_words_es<-c(stop_words_es$V1,FilPal$X1,correcciones_df$Palabra_Original)
stop_words_es <- tibble(word = unlist(stop_words_es), lexicon = "custom")
subtitulos_JEP_03<-subtitulos_JEP_03%>%
mutate(Subtitulos= str_replace_all(Subtitulos, "\\[Música\\]", ""))  %>%
filter(!is.na(Subtitulos) ) %>%
as.data.frame()
# Preprocesar los subtítulos
subtitulos_JEP_03 <- within(subtitulos_JEP_03, {
Subtitulos <- str_to_lower(Subtitulos)  # Convertir a minúsculas
Subtitulos <- str_replace_all(Subtitulos, "[^a-záéíóúüñ ]", "")  # Eliminar caracteres no alfabéticos
Subtitulos <- str_squish(Subtitulos)  # Eliminar espacios en blanco adicionales
})
#correr esta linea para hacer subcasos
subtitulos= paste(subtitulos_JEP_03$Subtitulos, collapse = ' ')
subtitulos <- tibble(text = subtitulos)
subtitulos <- subtitulos %>%
unnest_tokens(input = text, output = word, token = "regex", pattern = "\\s+")%>%
anti_join(x = ., y = stop_words_es)
# Extract tokens
tokens <- subtitulos$word
# Create bigrams from tokens and avoid multiple edges
bigrams <- sapply(1:(length(tokens) - 1), function(i) {
words <- sort(c(tokens[i], tokens[i + 1]))
paste(words, collapse = " ")
})
# Convert bigrams to a vector
bigrams <- as.vector(bigrams)
# Convert bigrams to dataframe
bigrams<- str_split_fixed(bigrams, " ", 2)
word_1 <- bigrams[, 1]
word_2 <- bigrams[, 2]
bigrams2 <- cbind(word_1, word_2)
bigrams2 <- as.data.frame(bigrams2)
bigrams2<-bigrams2%>%
filter(!word_1 %in% stop_words_es$word & !word_2 %in% stop_words_es$word)%>%
filter(word_1 != word_2)%>%
count(word_1, word_2, sort = TRUE) %>%
rename(weight = n)
# Calculate skewness for different thresholds
threshold <- unique(bigrams2$weight)
count <- bigrams2$weight
library(EnvStats)
s <- NULL
for (i in 1:length(threshold)) {
s[i] <- skewness(bigrams2[count > threshold[i], ]$weight)
#hist(bigrams[count > threshold[i], ]$n)
}
# Plot skewness vs threshold
plot(threshold, s,
xlim = c(0, 50),
ylim = range(s, na.rm = TRUE),
type = 'b',                   # Connect points with lines
pch = 19,                     # Point character
col = 'blue',                 # Point color
xlab = 'Threshold',           # X-axis label
ylab = 'Skewness',            # Y-axis label
main = '',  # Title
cex.main = 1.5,               # Title size
cex.lab = 1.2,                # Axis label size
cex.axis = 1.1,               # Axis tick label size
cex = 0.5)                    # Point size
#curve(45 / sqrt(x), from = 0.1, to = 100, add = TRUE, col = 'red', lwd = 2)
# Adding grid lines
grid(nx = NULL, ny = NULL, col = 'gray', lty = 'dotted')
# Adding a horizontal line at y=0 for reference
abline(v =3, col = 'red', lty = 2)
suppressMessages(suppressWarnings(library(igraph)))
n<-40
g <- bigrams2%>%
filter(weight > n) %>%
select(word_1,word_2)%>%
graph_from_data_frame(directed = FALSE)
# Find the largest connected component
components <- igraph::clusters(g, mode = "weak")
biggest_cluster_id <- which.max(components$csize)
vert_ids <- V(g)[components$membership == biggest_cluster_id]
# Create subgraph of the largest component
g2 <- igraph::induced_subgraph(g, vert_ids)
summary(g2)
num_nodes <- vcount(g2)
num_edges <- ecount(g2)
# Crear un dataframe para almacenar esta información
network_summary <- data.frame(
Num_Nodes = num_nodes,
Num_Edges = num_edges,
N_Value = n
)
network_summary
# Guardar la tabla en un archivo CSV
write.table(network_summary, "network_summary.txt",sep = ",", row.names = FALSE, col.names = TRUE, fileEncoding = "UTF-8")
set.seed(42)
# List of community detection functions
community_methods <- list(
Edge_Betweenness = cluster_edge_betweenness,
Fast_Greedy = cluster_fast_greedy,
Infomap = cluster_infomap,
Label_Propagation = cluster_label_prop,
Leading_Eigen = cluster_leading_eigen,
Leiden = cluster_leiden,
Louvain = cluster_louvain,
Spinglass = cluster_spinglass,
Walktrap = cluster_walktrap
)
# Initialize a vector to store modularity values
modularity_values <- numeric(length(community_methods))
names(modularity_values) <- names(community_methods)
# Apply each community detection method and calculate modularity
for (method_name in names(community_methods)) {
# Try to apply each method and catch any errors
tryCatch({
# Apply the community detection method
community_detection <- community_methods[[method_name]](g2)
# Calculate modularity
modularity_values[method_name] <- modularity(community_detection)
}, error = function(e) {
# If an error occurs, print the error message and move to the next method
cat("Error with", method_name, ": ", e$message, "\n")
modularity_values[method_name] <- NA
})
}
